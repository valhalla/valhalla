#!/usr/bin/env python3

import argparse
import ctypes
from io import BytesIO
import json
import logging
import os
from pathlib import Path
import struct
import sys
import tarfile
from tarfile import BLOCKSIZE
from time import time
from typing import List, Tuple

# "<" prefix means little-endian and no alignment
# order is important! if uint64_t is not first, c++ will use padding bytes to unpack
INDEX_BIN_FORMAT = '<QLL'
INDEX_BIN_SIZE = struct.calcsize(INDEX_BIN_FORMAT)
INDEX_FILE = "index.bin"
# skip the first 40 bytes of the tile header
GRAPHTILE_SKIP_BYTES = struct.calcsize('<Q2f16cQ')
TRAFFIC_HEADER_SIZE = struct.calcsize('<2Q4I')
TRAFFIC_SPEED_SIZE = struct.calcsize('<Q')


class TileHeader(ctypes.Structure):
    """
    Resembles the uint64_t bit field at bytes 40 - 48 of the
    graphtileheader to get the directededgecount_.
    """
    _fields_ = [
        ("nodecount_", ctypes.c_ulonglong, 21),
        ("directededgecount_", ctypes.c_ulonglong, 21),
        ("predictedspeeds_count_", ctypes.c_ulonglong, 21),
        ("spare1_", ctypes.c_ulonglong, 1)
    ]


description = "Builds a tar extract from the tiles in mjolnir.tile_dir to the path specified in mjolnir.tile_extract."
parser = argparse.ArgumentParser(description=description)
parser.add_argument("-c", "--config", help="Absolute or relative path to the Valhalla config JSON.", type=Path)
parser.add_argument("-i", "--inline-config", help="Inline JSON config, will override --config JSON if present", type=str, default='{}')
parser.add_argument("-t", "--with-traffic", help="Flag to add a traffic.tar skeleton", action="store_true", default=False)
parser.add_argument("-v", "--verbosity", help="Accumulative verbosity flags; -v: INFO, -vv: DEBUG", action='count', default=0)

# set up the logger basics
LOGGER = logging.getLogger(__name__)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)5s: %(message)s"))
LOGGER.addHandler(handler)


def get_tile_count(in_path: Path) -> int:
    """Iterates over the full tree and returns the count of all tiles it found."""
    count = 0
    for _, _, files in os.walk(in_path):
        count += len(list(filter(lambda f: f.endswith('.gph'), files)))

    return count


def get_tile_id(path: str) -> int:
    """Turns a tile path into a numeric GraphId"""
    level, idx = path[:-4].split('/', 1)

    return int(level) | (int(idx.replace('/', '')) << 3)


def get_tar_info(name: str, size: int) -> tarfile.TarInfo:
    """Creates and returns a tarinfo object"""
    tarinfo = tarfile.TarInfo(name)
    tarinfo.size = size
    tarinfo.mtime = int(time())
    tarinfo.type = tarfile.REGTYPE

    return tarinfo


def write_index_to_tar(tar_fp_: Path):
    """Loop through all tiles and write the correct index.bin file to the tar"""
    # get the offset and size from the tarred tile members
    index: List[Tuple[int, int, int]] = list()
    with tarfile.open(tar_fp_, 'r|') as tar:
        for member in tar.getmembers():
            if member.name.endswith('.gph'):
                LOGGER.debug(f"Tile {member.name} with offset: {member.offset_data}, size: {member.size}")

                index.append((member.offset_data, get_tile_id(member.name), member.size))

    # write back the actual index info
    with open(tar_fp_, 'r+b') as tar:
        # jump to the data block, index.bin is the first file
        tar.seek(BLOCKSIZE)
        for entry in index:
            tar.write(struct.pack(INDEX_BIN_FORMAT, *entry))


def create_extracts(config_: dict, do_traffic: bool):
    """Actually creates the tar ball. Break out of main function for testability."""
    tiles_fp: Path = Path(config_["mjolnir"].get("tile_dir", '/dev/null'))
    extract_fp: Path = Path(config_["mjolnir"].get("tile_extract") or tiles_fp.parent.joinpath('tiles.tar'))
    traffic_fp: Path = Path(config_["mjolnir"].get("traffic_extract") or tiles_fp.parent.joinpath('traffic.tar'))

    if not tiles_fp.is_dir():
        LOGGER.critical(f"Directory 'mjolnir.tile_dir': {tiles_fp.resolve()} was not found on the filesystem.")
        sys.exit(1)

    tiles_count = get_tile_count(tiles_fp)
    if not tiles_count:
        LOGGER.critical(f"Directory {tiles_fp} does not contain any usable graph tiles.")
        sys.exit(1)

    # write the in-memory index file
    index_size = INDEX_BIN_SIZE * tiles_count
    index_fd = BytesIO(b'0' * index_size)
    index_fd.seek(0)

    # first add the index file, then the sorted tiles to the tarfile
    # TODO: come up with a smarter strategy to cluster the tiles in the tar
    with tarfile.open(extract_fp, 'w') as tar:
        tar.addfile(get_tar_info(INDEX_FILE, index_size), index_fd)
        for t in sorted(tiles_fp.rglob('*.gph')):
            tar.add(str(t.resolve()), arcname=str(t.relative_to(tiles_fp)))

    write_index_to_tar(extract_fp)

    LOGGER.info(f"Finished tarring {tiles_count} tiles to {extract_fp}")

    # exit if no traffic extract wanted
    if not do_traffic:
        index_fd.close()
        sys.exit(0)

    LOGGER.info(f"Start creating traffic extract...")

    # we already have the right size of the index file, simply reset it
    index_fd.seek(0)
    with tarfile.open(extract_fp) as tar_in, tarfile.open(traffic_fp, 'w') as tar_traffic:
        # this will let us do seeks
        in_fileobj = tar_in.fileobj

        # add the index file as first data
        tar_traffic.addfile(get_tar_info(INDEX_FILE, index_size), index_fd)
        index_fd.close()

        # loop over all routing tiles and create fixed-size traffic tiles
        # based on the directed edge count
        for tile_in in tar_in.getmembers():
            if not tile_in.name.endswith('.gph'):
                continue
            # jump to the data's offset and skip the uninteresting bytes
            in_fileobj.seek(tile_in.offset_data + GRAPHTILE_SKIP_BYTES)

            # read the appropriate size of bytes from the tar into the TileHeader struct
            tile_header = TileHeader()
            b = BytesIO(in_fileobj.read(ctypes.sizeof(TileHeader)))
            b.readinto(tile_header)
            b.close()

            # create the traffic tile
            traffic_size = TRAFFIC_HEADER_SIZE + TRAFFIC_SPEED_SIZE * tile_header.directededgecount_
            tar_traffic.addfile(get_tar_info(tile_in.name, traffic_size), BytesIO(b'\0' * traffic_size))

            LOGGER.debug(f"Tile {tile_in.name} has {tile_header.directededgecount_} directed edges")

    write_index_to_tar(traffic_fp)

    LOGGER.info(f"Finished creating the traffic extract at {traffic_fp}")


if __name__ == '__main__':
    args = parser.parse_args()

    if not args.config and not args.inline_config:
        LOGGER.critical("No valid config file or inline config used.")
        sys.exit(1)

    config = dict()
    try:
        with open(args.config) as f:
            config = json.load(f)
    except TypeError:
        LOGGER.warning("Only inline-config will be used.")
    
    # override with inline-config
    config.update(**json.loads(args.inline_config))

    # set the right logger level
    if args.verbosity == 0:
        LOGGER.setLevel(logging.CRITICAL)
    elif args.verbosity == 1:
        LOGGER.setLevel(logging.INFO)
    elif args.verbosity >= 2:
        LOGGER.setLevel(logging.DEBUG)

    create_extracts(config, args.with_traffic)
