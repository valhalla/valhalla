#!/usr/bin/env python3

import argparse
from collections import namedtuple
import ctypes
from io import BytesIO
import json
from math import floor
import logging
import os
from pathlib import Path
import struct
import sys
import tarfile
from tarfile import BLOCKSIZE
from time import time
from typing import List, Tuple

# "<" prefix means little-endian and no alignment
# order is important! if uint64_t is not first, c++ will use padding bytes to unpack
INDEX_BIN_FORMAT = '<QLL'
INDEX_BIN_SIZE = struct.calcsize(INDEX_BIN_FORMAT)
INDEX_FILE = "index.bin"
# skip the first 40 bytes of the tile header
GRAPHTILE_SKIP_BYTES = struct.calcsize('<Q2f16cQ')
TRAFFIC_HEADER_SIZE = struct.calcsize('<2Q4I')
TRAFFIC_SPEED_SIZE = struct.calcsize('<Q')

Bbox = namedtuple("Bbox", "min_x min_y max_x max_y")
TILE_SIZES = {0: 4, 1: 1, 2: 0.25}


class TileHeader(ctypes.Structure):
    """
    Resembles the uint64_t bit field at bytes 40 - 48 of the
    graphtileheader to get the directededgecount_.
    """

    _fields_ = [
        ("nodecount_", ctypes.c_ulonglong, 21),
        ("directededgecount_", ctypes.c_ulonglong, 21),
        ("predictedspeeds_count_", ctypes.c_ulonglong, 21),
        ("spare1_", ctypes.c_ulonglong, 1),
    ]


description = "Builds a tar extract from the tiles in mjolnir.tile_dir to the path specified in mjolnir.tile_extract."
parser = argparse.ArgumentParser(description=description)
parser.add_argument(
    "-c", "--config", help="Absolute or relative path to the Valhalla config JSON.", type=Path
)
parser.add_argument(
    "-i",
    "--inline-config",
    help="Inline JSON config, will override --config JSON if present",
    type=str,
    default='{}',
)
parser.add_argument(
    "-t", "--with-traffic", help="Flag to add a traffic.tar skeleton", action="store_true", default=False
)
parser.add_argument(
    "-b",
    "--bbox",
    help="If specified, will only archive tiles which are intersecting this bbox. In 'minx,miny,maxx,maxy' format.",
    type=str,
    default="",
)
parser.add_argument(
    "-v",
    "--verbosity",
    help="Accumulative verbosity flags; -v: INFO, -vv: DEBUG",
    action='count',
    default=0,
)

# set up the logger basics
LOGGER = logging.getLogger(__name__)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)5s: %(message)s"))
LOGGER.addHandler(handler)


def tile_intersects_bbox(tile_path: str, in_bounds: Bbox) -> bool:
    """Check if a tile intersects the bbox"""
    if not in_bounds:
        return True

    level, tile_idx = [int(x.replace("/", "")) for x in get_tile_level_id(tile_path)]

    tile_size = TILE_SIZES[level]
    row = floor(tile_idx / (360 / tile_size))
    col = tile_idx % (360 / tile_size)

    tile_base_y = (row * tile_size) - 90
    tile_base_x = (col * tile_size) - 180
    tile_bbox = Bbox(
        tile_base_x,
        tile_base_y,
        tile_base_x + tile_size,
        tile_base_y + tile_size,
    )

    # check if tile_bbox is outside of in_bounds
    return not any(
        [
            tile_bbox.min_x < in_bounds.min_x
            and tile_bbox.max_x < in_bounds.min_x,  # bbox1 left of bbox2
            tile_bbox.min_y < in_bounds.min_y and tile_bbox.max_y < in_bounds.min_y,  # bbox1 below bbox2
            tile_bbox.min_x > in_bounds.max_x
            and tile_bbox.max_x > in_bounds.max_x,  # bbox1 right of bbox2
            tile_bbox.min_y > in_bounds.max_y and tile_bbox.max_y > in_bounds.max_y,  # bbox1 above bbox2
        ]
    )


def get_tile_count(in_path: Path) -> int:
    """Iterates over the full tree and returns the count of all tiles it found."""
    count = 0
    for _, _, files in os.walk(in_path):
        count += len(list(filter(lambda f: f.endswith('.gph'), files)))

    return count


def get_tile_level_id(path: str) -> Tuple[int, int]:
    """Returns both level and tile ID"""
    return path[:-4].split('/', 1)


def get_tile_id(path: str) -> int:
    """Turns a tile path into a numeric GraphId, including the level"""
    level, idx = get_tile_level_id(path)

    return int(level) | (int(idx.replace('/', '')) << 3)


def get_tar_info(name: str, size: int) -> tarfile.TarInfo:
    """Creates and returns a tarinfo object"""
    tarinfo = tarfile.TarInfo(name)
    tarinfo.size = size
    tarinfo.mtime = int(time())
    tarinfo.type = tarfile.REGTYPE

    return tarinfo


def write_index_to_tar(tar_fp_: Path):
    """Loop through all tiles and write the correct index.bin file to the tar"""
    # get the offset and size from the tarred tile members
    index: List[Tuple[int, int, int]] = list()
    with tarfile.open(tar_fp_, 'r|') as tar:
        for member in tar.getmembers():
            if member.name.endswith('.gph'):
                LOGGER.debug(
                    f"Tile {member.name} with offset: {member.offset_data}, size: {member.size}"
                )

                index.append((member.offset_data, get_tile_id(member.name), member.size))

    # write back the actual index info
    with open(tar_fp_, 'r+b') as tar:
        # jump to the data block, index.bin is the first file
        tar.seek(BLOCKSIZE)
        for entry in index:
            tar.write(struct.pack(INDEX_BIN_FORMAT, *entry))


def create_extracts(config_: dict, do_traffic: bool, bbox: Bbox):
    """Actually creates the tar ball. Break out of main function for testability."""
    tiles_fp: Path = Path(config_["mjolnir"].get("tile_dir", '/dev/null'))
    extract_fp: Path = Path(
        config_["mjolnir"].get("tile_extract") or tiles_fp.parent.joinpath('tiles.tar')
    )
    traffic_fp: Path = Path(
        config_["mjolnir"].get("traffic_extract") or tiles_fp.parent.joinpath('traffic.tar')
    )

    if not tiles_fp.is_dir():
        LOGGER.critical(
            f"Directory 'mjolnir.tile_dir': {tiles_fp.resolve()} was not found on the filesystem."
        )
        sys.exit(1)

    tiles_count = get_tile_count(tiles_fp)
    if not tiles_count:
        LOGGER.critical(f"Directory {tiles_fp} does not contain any usable graph tiles.")
        sys.exit(1)

    # write the in-memory index file
    index_size = INDEX_BIN_SIZE * tiles_count
    index_fd = BytesIO(b'0' * index_size)
    index_fd.seek(0)

    # first add the index file, then the sorted tiles to the tarfile
    # TODO: come up with a smarter strategy to cluster the tiles in the tar
    with tarfile.open(extract_fp, 'w') as tar:
        tar.addfile(get_tar_info(INDEX_FILE, index_size), index_fd)
        for t in sorted(tiles_fp.rglob('*.gph')):
            # only include the tiles falling inside the bbox
            rel_path = t.relative_to(tiles_fp)
            if not tile_intersects_bbox(rel_path, bbox):
                LOGGER.debug(f"Tile {rel_path} is outside {tuple(bbox)}.")
                continue
            LOGGER.debug(f"Adding tile {rel_path} to the path")
            tar.add(str(t.resolve()), arcname=str(rel_path))

    write_index_to_tar(extract_fp)

    LOGGER.info(f"Finished tarring {tiles_count} tiles to {extract_fp}")

    # exit if no traffic extract wanted
    if not do_traffic:
        index_fd.close()
        sys.exit(0)

    LOGGER.info("Start creating traffic extract...")

    # we already have the right size of the index file, simply reset it
    index_fd.seek(0)
    with tarfile.open(extract_fp) as tar_in, tarfile.open(traffic_fp, 'w') as tar_traffic:
        # this will let us do seeks
        in_fileobj = tar_in.fileobj

        # add the index file as first data
        tar_traffic.addfile(get_tar_info(INDEX_FILE, index_size), index_fd)
        index_fd.close()

        # loop over all routing tiles and create fixed-size traffic tiles
        # based on the directed edge count
        for tile_in in tar_in.getmembers():
            if not tile_in.name.endswith('.gph'):
                continue
            # jump to the data's offset and skip the uninteresting bytes
            in_fileobj.seek(tile_in.offset_data + GRAPHTILE_SKIP_BYTES)

            # read the appropriate size of bytes from the tar into the TileHeader struct
            tile_header = TileHeader()
            b = BytesIO(in_fileobj.read(ctypes.sizeof(TileHeader)))
            b.readinto(tile_header)
            b.close()

            # create the traffic tile
            traffic_size = TRAFFIC_HEADER_SIZE + TRAFFIC_SPEED_SIZE * tile_header.directededgecount_
            tar_traffic.addfile(get_tar_info(tile_in.name, traffic_size), BytesIO(b'\0' * traffic_size))

            LOGGER.debug(f"Tile {tile_in.name} has {tile_header.directededgecount_} directed edges")

    write_index_to_tar(traffic_fp)

    LOGGER.info(f"Finished creating the traffic extract at {traffic_fp}")


if __name__ == '__main__':
    args = parser.parse_args()

    if not args.config and not args.inline_config:
        LOGGER.critical("No valid config file or inline config used.")
        sys.exit(1)

    config = dict()
    try:
        with open(args.config) as f:
            config = json.load(f)
    except TypeError:
        LOGGER.warning("Only inline-config will be used.")

    # override with inline-config
    config.update(**json.loads(args.inline_config))

    # see if there's a bbox to limit the tiles archived
    bbox = []
    if args.bbox:
        try:
            bbox = Bbox(*[float(x) for x in args.bbox.split(",")])
        except ValueError:
            LOGGER.critical(f"BBOX {args.bbox} is not a comma-separated string of coordinates.")
            sys.exit(1)

        # validate bbox
        if (bbox.min_x >= bbox.max_x or bbox.min_x < -180 or bbox.max_x > 180) or (
            bbox.min_y >= bbox.max_y or bbox.min_y < -90 or bbox.max_y > 90
        ):
            LOGGER.critical(f"Bbox invalid: {list(bbox)}")
            sys.exit(1)

    # set the right logger level
    if args.verbosity == 0:
        LOGGER.setLevel(logging.CRITICAL)
    elif args.verbosity == 1:
        LOGGER.setLevel(logging.INFO)
    elif args.verbosity >= 2:
        LOGGER.setLevel(logging.DEBUG)

    create_extracts(config, args.with_traffic, bbox)
